{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df671b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4784ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dba921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898034b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42454238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc901c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c37308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935ff67",
   "metadata": {},
   "source": [
    "# downlaod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fef677",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname = 'spa-eng.zip',\n",
    "    origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True,\n",
    "    )\n",
    "\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7677d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(text_file, encoding='cp437')\n",
    "lines = f.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "243d9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77154a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"If it rains tomorrow, I'm not going to the meeting.\", '[start] Si llueve ma├▒ana, no ir├⌐ a la reuni├│n. [end]')\n",
      "('The guests wished the happy couple a long and prosperous life.', '[start] Los invitados desearon a la feliz pareja una larga y pr├│spera vida. [end]')\n",
      "('Tom and Mary both like old movies.', '[start] A Tom y a Mary les gustan las pel├¡culas antiguas. [end]')\n",
      "('I want to meet with Tom.', '[start] Quiero encontrarme con Tom. [end]')\n",
      "('No one knows.', '[start] Nadie sabe. [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab8dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(\n",
    "    text_pairs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "781a4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_samples = int(0.15*len(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095e4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = len(text_pairs) - 2*num_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69acd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = text_pairs[:num_train_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd17aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "147d0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b938b2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee7404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17844"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9d84a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17844"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0784b",
   "metadata": {},
   "source": [
    "# vectorizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5b198db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~¿'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_chars = string.punctuation+\"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\",\"\")\n",
    "strip_chars = strip_chars.replace(\"]\",\"\")\n",
    "strip_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5418114",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a28a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(\n",
    "    input_string,\n",
    "    ):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase,\n",
    "        \"[%s]\"% re.escape(strip_chars),\n",
    "        \"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c28e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hi how are you'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_standardization(\"hi, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5e4320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length+1,\n",
    "    standardize=custom_standardization\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c60959",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b307bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization.adapt(train_eng_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0226efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ac9c2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "array([[ 16,   8,   7, 881,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorization([\"This is a test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae4ceb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12042"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorization.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9369a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'a',\n",
       " 'is',\n",
       " 'he',\n",
       " 'in',\n",
       " 'of',\n",
       " 'it',\n",
       " 'that',\n",
       " 'was',\n",
       " 'do',\n",
       " 'this',\n",
       " 'me',\n",
       " 'have',\n",
       " 'my',\n",
       " 'for',\n",
       " 'she',\n",
       " 'dont',\n",
       " 'are',\n",
       " 'what',\n",
       " 'his',\n",
       " 'we',\n",
       " 'mary',\n",
       " 'your',\n",
       " 'on',\n",
       " 'be',\n",
       " 'with',\n",
       " 'want',\n",
       " 'not',\n",
       " 'im',\n",
       " 'and',\n",
       " 'at',\n",
       " 'like',\n",
       " 'know',\n",
       " 'him',\n",
       " 'can',\n",
       " 'go',\n",
       " 'her',\n",
       " 'will',\n",
       " 'has',\n",
       " 'there',\n",
       " 'its',\n",
       " 'they',\n",
       " 'time',\n",
       " 'as',\n",
       " 'how',\n",
       " 'very',\n",
       " 'did',\n",
       " 'were',\n",
       " 'had',\n",
       " 'all',\n",
       " 'here',\n",
       " 'about',\n",
       " 'up',\n",
       " 'think',\n",
       " 'didnt',\n",
       " 'get',\n",
       " 'when',\n",
       " 'out',\n",
       " 'from',\n",
       " 'cant',\n",
       " 'if',\n",
       " 'an',\n",
       " 'no',\n",
       " 'doesnt',\n",
       " 'one',\n",
       " 'would',\n",
       " 'going',\n",
       " 'by',\n",
       " 'why',\n",
       " 'see',\n",
       " 'come',\n",
       " 'good',\n",
       " 'ill',\n",
       " 'please',\n",
       " 'youre',\n",
       " 'who',\n",
       " 'just',\n",
       " 'been',\n",
       " 'so',\n",
       " 'need',\n",
       " 'more',\n",
       " 'but',\n",
       " 'help',\n",
       " 'tell',\n",
       " 'now',\n",
       " 'where',\n",
       " 'never',\n",
       " 'than',\n",
       " 'us',\n",
       " 'am',\n",
       " 'got',\n",
       " 'some',\n",
       " 'last',\n",
       " 'something',\n",
       " 'take',\n",
       " 'ive',\n",
       " 'should',\n",
       " 'too',\n",
       " 'could',\n",
       " 'much',\n",
       " 'car',\n",
       " 'day',\n",
       " 'money',\n",
       " 'home',\n",
       " 'people',\n",
       " 'work',\n",
       " 'well',\n",
       " 'really',\n",
       " 'many',\n",
       " 'said',\n",
       " 'told',\n",
       " 'back',\n",
       " 'house',\n",
       " 'went',\n",
       " 'our',\n",
       " 'lot',\n",
       " 'anything',\n",
       " 'any',\n",
       " 'say',\n",
       " 'french',\n",
       " 'book',\n",
       " 'does',\n",
       " 'always',\n",
       " 'make',\n",
       " 'isnt',\n",
       " 'speak',\n",
       " 'hes',\n",
       " 'school',\n",
       " 'only',\n",
       " 'eat',\n",
       " 'thats',\n",
       " 'thought',\n",
       " 'room',\n",
       " 'new',\n",
       " 'give',\n",
       " 'long',\n",
       " 'today',\n",
       " 'made',\n",
       " 'two',\n",
       " 'toms',\n",
       " 'must',\n",
       " 'right',\n",
       " 'old',\n",
       " 'every',\n",
       " 'love',\n",
       " 'father',\n",
       " 'night',\n",
       " 'three',\n",
       " 'look',\n",
       " 'wanted',\n",
       " 'man',\n",
       " 'tomorrow',\n",
       " 'before',\n",
       " 'talk',\n",
       " 'off',\n",
       " 'still',\n",
       " 'id',\n",
       " 'lets',\n",
       " 'let',\n",
       " 'asked',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'little',\n",
       " 'boston',\n",
       " 'saw',\n",
       " 'way',\n",
       " 'years',\n",
       " 'after',\n",
       " 'over',\n",
       " 'dog',\n",
       " 'may',\n",
       " 'yesterday',\n",
       " 'or',\n",
       " 'put',\n",
       " 'whats',\n",
       " 'nothing',\n",
       " 'english',\n",
       " 'again',\n",
       " 'better',\n",
       " 'job',\n",
       " 'into',\n",
       " 'live',\n",
       " 'down',\n",
       " 'them',\n",
       " 'read',\n",
       " 'everything',\n",
       " 'happy',\n",
       " 'stay',\n",
       " 'wants',\n",
       " 'came',\n",
       " 'buy',\n",
       " 'first',\n",
       " 'couldnt',\n",
       " 'wont',\n",
       " 'life',\n",
       " 'find',\n",
       " 'mother',\n",
       " 'other',\n",
       " 'theres',\n",
       " 'next',\n",
       " 'play',\n",
       " 'children',\n",
       " 'took',\n",
       " 'friends',\n",
       " 'understand',\n",
       " 'bought',\n",
       " 'away',\n",
       " 'call',\n",
       " 'morning',\n",
       " 'ask',\n",
       " 'these',\n",
       " 'doing',\n",
       " 'used',\n",
       " 'stop',\n",
       " 'door',\n",
       " 'believe',\n",
       " 'keep',\n",
       " 'ever',\n",
       " 'soon',\n",
       " 'gave',\n",
       " 'late',\n",
       " 'their',\n",
       " 'problem',\n",
       " 'lost',\n",
       " 'feel',\n",
       " 'happened',\n",
       " 'remember',\n",
       " 'friend',\n",
       " 'sure',\n",
       " 'without',\n",
       " 'name',\n",
       " 'enough',\n",
       " 'already',\n",
       " 'bed',\n",
       " 'hard',\n",
       " 'year',\n",
       " 'likes',\n",
       " 'even',\n",
       " 'alone',\n",
       " 'busy',\n",
       " 'things',\n",
       " 'done',\n",
       " 'teacher',\n",
       " 'same',\n",
       " 'wasnt',\n",
       " 'wrong',\n",
       " 'married',\n",
       " 'week',\n",
       " 'try',\n",
       " 'water',\n",
       " 'party',\n",
       " 'heard',\n",
       " 'best',\n",
       " 'marys',\n",
       " 'looks',\n",
       " 'knows',\n",
       " 'looking',\n",
       " 'beautiful',\n",
       " 'seen',\n",
       " 'bad',\n",
       " 'cold',\n",
       " 'person',\n",
       " 'thing',\n",
       " 'often',\n",
       " 'because',\n",
       " 'hear',\n",
       " 'boy',\n",
       " 'wait',\n",
       " 'being',\n",
       " 'idea',\n",
       " 'knew',\n",
       " 'use',\n",
       " 'around',\n",
       " 'answer',\n",
       " 'able',\n",
       " 'tired',\n",
       " 'brother',\n",
       " 'while',\n",
       " 'train',\n",
       " 'big',\n",
       " 'food',\n",
       " 'books',\n",
       " 'letter',\n",
       " 'found',\n",
       " 'learn',\n",
       " 'drink',\n",
       " 'theyre',\n",
       " 'days',\n",
       " 'john',\n",
       " 'true',\n",
       " 'coffee',\n",
       " 'watch',\n",
       " 'japan',\n",
       " 'met',\n",
       " 'almost',\n",
       " 'hope',\n",
       " 'girl',\n",
       " 'which',\n",
       " 'everyone',\n",
       " 'bus',\n",
       " 'open',\n",
       " 'yet',\n",
       " 'wish',\n",
       " 'shes',\n",
       " 'pay',\n",
       " 'those',\n",
       " 'sleep',\n",
       " 'few',\n",
       " 'kind',\n",
       " 'study',\n",
       " 'each',\n",
       " 'parents',\n",
       " 'youll',\n",
       " 'ago',\n",
       " 'lunch',\n",
       " 'truth',\n",
       " 'place',\n",
       " 'ten',\n",
       " 'mind',\n",
       " 'walk',\n",
       " 'talking',\n",
       " 'afraid',\n",
       " 'young',\n",
       " 'world',\n",
       " 'most',\n",
       " 'looked',\n",
       " 'great',\n",
       " 'early',\n",
       " 'doctor',\n",
       " 'care',\n",
       " 'someone',\n",
       " 'seems',\n",
       " 'havent',\n",
       " 'getting',\n",
       " 'anyone',\n",
       " 'turn',\n",
       " 'died',\n",
       " 'both',\n",
       " 'once',\n",
       " 'tried',\n",
       " 'family',\n",
       " 'together',\n",
       " 'tv',\n",
       " 'difficult',\n",
       " 'sister',\n",
       " 'favorite',\n",
       " 'else',\n",
       " 'youve',\n",
       " 'own',\n",
       " 'mine',\n",
       " 'happen',\n",
       " 'show',\n",
       " 'another',\n",
       " 'until',\n",
       " 'child',\n",
       " 'yourself',\n",
       " 'plan',\n",
       " 'write',\n",
       " 'cat',\n",
       " 'since',\n",
       " 'wouldnt',\n",
       " 'reading',\n",
       " 'question',\n",
       " 'might',\n",
       " 'angry',\n",
       " 'police',\n",
       " 'window',\n",
       " 'monday',\n",
       " 'meeting',\n",
       " 'dinner',\n",
       " 'sorry',\n",
       " 'arent',\n",
       " 'waiting',\n",
       " 'five',\n",
       " 'rain',\n",
       " 'coming',\n",
       " 'accident',\n",
       " 'nobody',\n",
       " 'wife',\n",
       " 'lives',\n",
       " 'everybody',\n",
       " 'says',\n",
       " 'hours',\n",
       " 'meet',\n",
       " 'ate',\n",
       " 'matter',\n",
       " 'himself',\n",
       " 'hand',\n",
       " 'playing',\n",
       " 'phone',\n",
       " 'japanese',\n",
       " 'hate',\n",
       " 'trying',\n",
       " 'office',\n",
       " 'arrived',\n",
       " 'ran',\n",
       " 'such',\n",
       " 'important',\n",
       " 'minutes',\n",
       " 'hurt',\n",
       " 'fire',\n",
       " 'finished',\n",
       " 'swim',\n",
       " 'son',\n",
       " 'nice',\n",
       " 'month',\n",
       " 'felt',\n",
       " 'eating',\n",
       " 'called',\n",
       " 'small',\n",
       " 'drive',\n",
       " 'under',\n",
       " 'table',\n",
       " 'start',\n",
       " 'ready',\n",
       " 'country',\n",
       " 'students',\n",
       " 'working',\n",
       " 'times',\n",
       " 'story',\n",
       " 'decided',\n",
       " 'far',\n",
       " 'change',\n",
       " 'myself',\n",
       " 'tonight',\n",
       " 'easy',\n",
       " 'city',\n",
       " 'trouble',\n",
       " 'pretty',\n",
       " 'park',\n",
       " 'needs',\n",
       " 'usually',\n",
       " 'started',\n",
       " 'shoes',\n",
       " 'having',\n",
       " 'station',\n",
       " 'mistake',\n",
       " 'fast',\n",
       " 'town',\n",
       " 'shouldnt',\n",
       " 'christmas',\n",
       " 'anymore',\n",
       " 'woman',\n",
       " 'music',\n",
       " 'broke',\n",
       " 'six',\n",
       " 'homework',\n",
       " 'goes',\n",
       " 'gone',\n",
       " 'baby',\n",
       " 'sick',\n",
       " 'visit',\n",
       " 'tennis',\n",
       " 'hot',\n",
       " 'picture',\n",
       " 'afternoon',\n",
       " 'fun',\n",
       " 'word',\n",
       " 'run',\n",
       " 'game',\n",
       " 'song',\n",
       " 'language',\n",
       " 'forget',\n",
       " 'eyes',\n",
       " 'cake',\n",
       " 'breakfast',\n",
       " 'movie',\n",
       " 'whos',\n",
       " 'cannot',\n",
       " 'thank',\n",
       " 'stupid',\n",
       " 'spend',\n",
       " 'red',\n",
       " 'lived',\n",
       " 'hair',\n",
       " 'class',\n",
       " 'turned',\n",
       " 'then',\n",
       " 'sit',\n",
       " 'listen',\n",
       " 'kept',\n",
       " 'studying',\n",
       " 'smoking',\n",
       " 'against',\n",
       " 'interesting',\n",
       " 'fell',\n",
       " 'caught',\n",
       " 'rich',\n",
       " 'makes',\n",
       " 'face',\n",
       " 'comes',\n",
       " 'quite',\n",
       " 'began',\n",
       " 'australia',\n",
       " 'along',\n",
       " 'weather',\n",
       " 'through',\n",
       " 'summer',\n",
       " 'hospital',\n",
       " 'free',\n",
       " 'close',\n",
       " 'mean',\n",
       " 'loves',\n",
       " 'fish',\n",
       " 'finish',\n",
       " 'america',\n",
       " 'weve',\n",
       " 'living',\n",
       " 'hotel',\n",
       " 'war',\n",
       " 'present',\n",
       " 'number',\n",
       " 'longer',\n",
       " 'large',\n",
       " 'milk',\n",
       " 'later',\n",
       " 'river',\n",
       " 'older',\n",
       " 'full',\n",
       " 'advice',\n",
       " 'short',\n",
       " 'making',\n",
       " 'sometimes',\n",
       " 'bicycle',\n",
       " 'became',\n",
       " 'seem',\n",
       " 'secret',\n",
       " 'news',\n",
       " 'cut',\n",
       " 'box',\n",
       " 'wonder',\n",
       " 'street',\n",
       " 'snow',\n",
       " 'hour',\n",
       " 'born',\n",
       " 'watching',\n",
       " 'thinking',\n",
       " 'sat',\n",
       " 'surprised',\n",
       " 'hurry',\n",
       " 'gets',\n",
       " 'dream',\n",
       " 'dollars',\n",
       " 'bring',\n",
       " 'killed',\n",
       " 'end',\n",
       " 'tree',\n",
       " 'forgot',\n",
       " 'age',\n",
       " 'youd',\n",
       " 'hungry',\n",
       " 'others',\n",
       " 'near',\n",
       " 'men',\n",
       " 'tokyo',\n",
       " 'student',\n",
       " 'high',\n",
       " 'death',\n",
       " 'dark',\n",
       " 'bit',\n",
       " 'trust',\n",
       " 'miss',\n",
       " 'birthday',\n",
       " 'white',\n",
       " 'stayed',\n",
       " 'rather',\n",
       " 'hands',\n",
       " 'dogs',\n",
       " 'camera',\n",
       " 'stand',\n",
       " 'sing',\n",
       " 'daughter',\n",
       " 'changed',\n",
       " 'works',\n",
       " 'whether',\n",
       " 'questions',\n",
       " 'oclock',\n",
       " 'light',\n",
       " 'dead',\n",
       " 'taking',\n",
       " 'saying',\n",
       " 'hasnt',\n",
       " 'company',\n",
       " 'beer',\n",
       " 'tea',\n",
       " 'stopped',\n",
       " 'lie',\n",
       " 'kill',\n",
       " 'computer',\n",
       " 'clean',\n",
       " 'spent',\n",
       " 'possible',\n",
       " 'moment',\n",
       " 'girlfriend',\n",
       " 'chance',\n",
       " 'careful',\n",
       " 'behind',\n",
       " '230',\n",
       " 'speaking',\n",
       " 'hit',\n",
       " 'glad',\n",
       " 'flowers',\n",
       " 'eaten',\n",
       " 'catch',\n",
       " 'wine',\n",
       " 'speaks',\n",
       " 'helped',\n",
       " 'become',\n",
       " 'wrote',\n",
       " 'wheres',\n",
       " 'order',\n",
       " 'die',\n",
       " 'business',\n",
       " 'worked',\n",
       " 'takes',\n",
       " 'sunday',\n",
       " 'strange',\n",
       " 'store',\n",
       " 'black',\n",
       " 'anybody',\n",
       " 'dress',\n",
       " 'canadian',\n",
       " 'whole',\n",
       " 'touch',\n",
       " 'thinks',\n",
       " 'piano',\n",
       " 'outside',\n",
       " 'four',\n",
       " 'drunk',\n",
       " 'break',\n",
       " 'team',\n",
       " 'plane',\n",
       " 'cats',\n",
       " 'bank',\n",
       " 'needed',\n",
       " 'maybe',\n",
       " 'lose',\n",
       " 'advised',\n",
       " 'yours',\n",
       " 'problems',\n",
       " 'floor',\n",
       " 'building',\n",
       " 'tall',\n",
       " 'part',\n",
       " 'paper',\n",
       " 'mistakes',\n",
       " 'expensive',\n",
       " 'between',\n",
       " 'wearing',\n",
       " 'paid',\n",
       " 'garden',\n",
       " 'exactly',\n",
       " 'blame',\n",
       " 'baseball',\n",
       " 'bag',\n",
       " 'women',\n",
       " 'trip',\n",
       " 'shirt',\n",
       " 'reason',\n",
       " 'least',\n",
       " 'different',\n",
       " 'cup',\n",
       " 'clothes',\n",
       " 'words',\n",
       " 'played',\n",
       " 'noise',\n",
       " 'interested',\n",
       " 'herself',\n",
       " 'front',\n",
       " 'famous',\n",
       " 'worry',\n",
       " 'waited',\n",
       " 'sound',\n",
       " 'send',\n",
       " 'running',\n",
       " 'probably',\n",
       " 'poor',\n",
       " 'pain',\n",
       " 'ice',\n",
       " 'hide',\n",
       " 'boys',\n",
       " 'air',\n",
       " 'prefer',\n",
       " 'walked',\n",
       " 'umbrella',\n",
       " 'telephone',\n",
       " 'seemed',\n",
       " 'hold',\n",
       " 'enjoy',\n",
       " 'during',\n",
       " 'dictionary',\n",
       " 'cost',\n",
       " 'safe',\n",
       " 'rest',\n",
       " 'quickly',\n",
       " 'opened',\n",
       " 'library',\n",
       " 'evening',\n",
       " 'dangerous',\n",
       " 'attention',\n",
       " 'asleep',\n",
       " 'strong',\n",
       " 'showed',\n",
       " 'sad',\n",
       " 'real',\n",
       " 'minute',\n",
       " 'key',\n",
       " 'hat',\n",
       " 'either',\n",
       " 'cry',\n",
       " 'arrive',\n",
       " 'weight',\n",
       " 'swimming',\n",
       " 'restaurant',\n",
       " 'movies',\n",
       " 'known',\n",
       " 'crazy',\n",
       " 'choice',\n",
       " 'agree',\n",
       " 'uncle',\n",
       " 'raining',\n",
       " 'ok',\n",
       " 'move',\n",
       " 'loved',\n",
       " 'learned',\n",
       " 'heart',\n",
       " 'head',\n",
       " 'feeling',\n",
       " 'several',\n",
       " 'seeing',\n",
       " 'return',\n",
       " 'quit',\n",
       " 'hell',\n",
       " 'danger',\n",
       " 'thirty',\n",
       " 'television',\n",
       " 'glasses',\n",
       " 'seven',\n",
       " 'rules',\n",
       " 'missed',\n",
       " 'guitar',\n",
       " 'cook',\n",
       " 'animals',\n",
       " 'win',\n",
       " 'wear',\n",
       " 'smoke',\n",
       " 'radio',\n",
       " 'lucky',\n",
       " 'invited',\n",
       " 'glass',\n",
       " 'forward',\n",
       " 'fishing',\n",
       " 'explain',\n",
       " 'drinking',\n",
       " 'winter',\n",
       " 'whose',\n",
       " 'talked',\n",
       " 'shut',\n",
       " 'pass',\n",
       " 'less',\n",
       " 'knife',\n",
       " 'kitchen',\n",
       " 'fine',\n",
       " 'finally',\n",
       " 'accept',\n",
       " 'visited',\n",
       " 'united',\n",
       " 'twice',\n",
       " 'traffic',\n",
       " 'situation',\n",
       " 'lawyer',\n",
       " 'half',\n",
       " 'broken',\n",
       " 'won',\n",
       " 'sun',\n",
       " 'somebody',\n",
       " 'set',\n",
       " 'road',\n",
       " 'proud',\n",
       " 'paris',\n",
       " 'husband',\n",
       " 'health',\n",
       " 'closed',\n",
       " 'side',\n",
       " 'dance',\n",
       " 'brought',\n",
       " 'though',\n",
       " 'thanks',\n",
       " 'spoke',\n",
       " 'shouldve',\n",
       " 'meat',\n",
       " 'lying',\n",
       " 'london',\n",
       " 'concert',\n",
       " 'writing',\n",
       " 'worried',\n",
       " 'wall',\n",
       " 'second',\n",
       " 'promise',\n",
       " 'passed',\n",
       " 'mountain',\n",
       " 'kiss',\n",
       " 'desk',\n",
       " 'beach',\n",
       " 'states',\n",
       " 'sent',\n",
       " 'seat',\n",
       " 'lake',\n",
       " 'foreign',\n",
       " 'follow',\n",
       " 'crying',\n",
       " 'worse',\n",
       " 'tie',\n",
       " 'solve',\n",
       " 'ship',\n",
       " 'sense',\n",
       " 'point',\n",
       " 'kids',\n",
       " 'guess',\n",
       " 'excuse',\n",
       " 'driving',\n",
       " 'decision',\n",
       " 'werent',\n",
       " 'wake',\n",
       " 'sleeping',\n",
       " 'opinion',\n",
       " 'months',\n",
       " 'learning',\n",
       " 'blue',\n",
       " 'across',\n",
       " 'worth',\n",
       " 'wash',\n",
       " 'vacation',\n",
       " 'travel',\n",
       " 'ticket',\n",
       " 'liked',\n",
       " 'girls',\n",
       " 'eggs',\n",
       " 'coat',\n",
       " 'clear',\n",
       " 'china',\n",
       " 'apple',\n",
       " 'airport',\n",
       " 'younger',\n",
       " 'weekend',\n",
       " 'waste',\n",
       " 'pick',\n",
       " 'patient',\n",
       " 'message',\n",
       " 'impossible',\n",
       " 'hardly',\n",
       " 'grandfather',\n",
       " 'completely',\n",
       " 'apples',\n",
       " 'sounds',\n",
       " 'smart',\n",
       " 'guy',\n",
       " 'bird',\n",
       " 'pictures',\n",
       " 'mad',\n",
       " 'leaving',\n",
       " 'hiding',\n",
       " 'fight',\n",
       " 'expect',\n",
       " 'abroad',\n",
       " 'written',\n",
       " 'teach',\n",
       " 'sitting',\n",
       " 'sign',\n",
       " 'piece',\n",
       " 'medicine',\n",
       " 'france',\n",
       " 'easily',\n",
       " 'drank',\n",
       " 'case',\n",
       " 'test',\n",
       " 'stood',\n",
       " 'save',\n",
       " 'machine',\n",
       " 'listening',\n",
       " 'languages',\n",
       " 'choose',\n",
       " 'chair',\n",
       " 'bread',\n",
       " 'begin',\n",
       " 'alive',\n",
       " 'taken',\n",
       " 'shot',\n",
       " 'happens',\n",
       " 'funny',\n",
       " 'american',\n",
       " 'smile',\n",
       " 'shopping',\n",
       " 'serious',\n",
       " 'none',\n",
       " 'newspaper',\n",
       " 'itll',\n",
       " 'gun',\n",
       " 'figure',\n",
       " 'eats',\n",
       " 'arm',\n",
       " 'wallet',\n",
       " 'terrible',\n",
       " 'somewhere',\n",
       " 'soccer',\n",
       " 'plans',\n",
       " 'perfect',\n",
       " 'mustve',\n",
       " 'keys',\n",
       " 'hed',\n",
       " 'expected',\n",
       " 'brothers',\n",
       " 'york',\n",
       " 'past',\n",
       " 'lend',\n",
       " 'horse',\n",
       " 'happening',\n",
       " 'future',\n",
       " 'suddenly',\n",
       " 'sell',\n",
       " 'quiet',\n",
       " 'necessary',\n",
       " 'inside',\n",
       " 'deal',\n",
       " 'cooking',\n",
       " 'chinese',\n",
       " 'trees',\n",
       " 'taxi',\n",
       " 'slowly',\n",
       " 'sky',\n",
       " 'shop',\n",
       " 'pen',\n",
       " 'offer',\n",
       " 'fault',\n",
       " 'count',\n",
       " 'also',\n",
       " 'sugar',\n",
       " 'singing',\n",
       " 'ride',\n",
       " 'public',\n",
       " 'promised',\n",
       " 'list',\n",
       " 'law',\n",
       " 'laughed',\n",
       " 'joke',\n",
       " 'enemy',\n",
       " 'doubt',\n",
       " 'cream',\n",
       " 'college',\n",
       " 'carry',\n",
       " 'ball',\n",
       " 'whatever',\n",
       " 'walking',\n",
       " 'voice',\n",
       " 'telling',\n",
       " 'spoken',\n",
       " 'policeman',\n",
       " 'moved',\n",
       " 'honest',\n",
       " 'guys',\n",
       " 'german',\n",
       " 'earth',\n",
       " 'couple',\n",
       " 'cars',\n",
       " 'bath',\n",
       " 'address',\n",
       " 'surprise',\n",
       " 'success',\n",
       " 'stolen',\n",
       " 'standing',\n",
       " 'refused',\n",
       " 'grew',\n",
       " 'feed',\n",
       " 'fat',\n",
       " 'empty',\n",
       " 'church',\n",
       " 'bridge',\n",
       " 'worst',\n",
       " 'sisters',\n",
       " 'received',\n",
       " 'novel',\n",
       " 'mouth',\n",
       " 'heavy',\n",
       " 'failed',\n",
       " 'experience',\n",
       " 'difference',\n",
       " 'decide',\n",
       " 'carefully',\n",
       " 'boyfriend',\n",
       " 'blood',\n",
       " 'wondered',\n",
       " 'storm',\n",
       " 'smiled',\n",
       " 'means',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09240610",
   "metadata": {},
   "source": [
    "# make the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8ea3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(\n",
    "    eng,\n",
    "    spa,\n",
    "    ):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return ({\n",
    "        \"encoder_inputs\": eng,\n",
    "        \"decoder_inputs\":spa[:,:-1]\n",
    "    }, spa[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75c74081",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = format_dataset(\n",
    "    train_eng_texts,\n",
    "    train_spa_texts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5256b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We elected him captain of our team.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eng_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c764673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] Lo elegimos capit├ín de nuestro equipo. [end]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spa_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de52a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([  26, 1962,   39, 1713,   11,  120,  636,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['encoder_inputs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e30a3561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([   2,   16, 6035, 1920,    4,  228,  579,    3,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['decoder_inputs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb27b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([  16, 6035, 1920,    4,  228,  579,    3,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b7f31",
   "metadata": {},
   "source": [
    "# make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e61e0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "e, s = zip(*train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35771bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It snowed in Osaka.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d15b64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] Nev├│ en Osaka. [end]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bfd091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    pairs,\n",
    "    ):\n",
    "    \n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    \n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        eng_texts,\n",
    "        spa_texts,\n",
    "        ))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)    \n",
    "    dataset = dataset.map(format_dataset)\n",
    "    dataset = dataset.shuffle(2048).prefetch(16).cache()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00fa8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62a2c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  3 137  26 ...   0   0   0]\n",
      " [  6   8 497 ...   0   0   0]\n",
      " [ 15   5 131 ...   0   0   0]\n",
      " ...\n",
      " [  6  44   4 ...   0   0   0]\n",
      " [ 19 202  44 ...   0   0   0]\n",
      " [ 91   8   2 ...   0   0   0]], shape=(64, 20), dtype=int64)\n",
      "\n",
      "tf.Tensor(\n",
      "[[   2  170    5 ...    0    0    0]\n",
      " [   2    8   12 ...    0    0    0]\n",
      " [   2 3812 5973 ...    0    0    0]\n",
      " ...\n",
      " [   2    8   43 ...    0    0    0]\n",
      " [   2   22  166 ...    0    0    0]\n",
      " [   2  147   23 ...    0    0    0]], shape=(64, 20), dtype=int64)\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 170    5 2339 ...    0    0    0]\n",
      " [   8   12  360 ...    0    0    0]\n",
      " [3812 5973    3 ...    0    0    0]\n",
      " ...\n",
      " [   8   43    5 ...    0    0    0]\n",
      " [  22  166   43 ...    0    0    0]\n",
      " [ 147   23   10 ...    0    0    0]], shape=(64, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs['encoder_inputs'])\n",
    "    print()\n",
    "    print(inputs['decoder_inputs'])\n",
    "    print()\n",
    "    print(targets)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c426eb8",
   "metadata": {},
   "source": [
    "# build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4351ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "sequence_length = 20\n",
    "vocab_size = 1500\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d824d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp import layers as nlp_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871d672",
   "metadata": {},
   "source": [
    "https://keras.io/api/keras_nlp/layers/token_and_position_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab4f6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_inputs (InputLayer)  [(None, 20)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 20, 256)          389120    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 20, 256)          395776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 784,896\n",
      "Trainable params: 784,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(\n",
    "    shape = (sequence_length,),\n",
    "    dtype = 'int64', \n",
    "    name = 'encoder_inputs',\n",
    "    )\n",
    "\n",
    "x = nlp_layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocab_size,\n",
    "    sequence_length=sequence_length,\n",
    "    embedding_dim=embed_dim,\n",
    "    )(encoder_inputs)\n",
    "\n",
    "encoder_outputs = nlp_layers.TransformerEncoder(\n",
    "    intermediate_dim = embed_dim,\n",
    "    num_heads = num_heads,\n",
    "    )(x)\n",
    "\n",
    "encoder = keras.Model(\n",
    "    encoder_inputs,\n",
    "    encoder_outputs\n",
    "    )\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d5ad665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 20, 256), dtype=float32, numpy=\n",
       "array([[[ 0.6090026 , -0.28974518,  1.5200865 , ...,  1.6366849 ,\n",
       "         -0.58401585,  0.7567553 ],\n",
       "        [-0.89278877, -2.0578983 , -0.9273504 , ...,  0.5028951 ,\n",
       "         -0.592074  ,  1.5509577 ],\n",
       "        [ 0.11033611, -1.4313439 ,  0.610609  , ...,  1.0276299 ,\n",
       "          0.44753313, -1.5974877 ],\n",
       "        ...,\n",
       "        [-1.7670937 , -0.48264557,  1.140495  , ..., -0.19191268,\n",
       "         -0.9674746 , -0.15440458],\n",
       "        [ 0.8025947 , -2.5771644 ,  0.86330795, ...,  0.36362442,\n",
       "         -0.23435518,  1.4771689 ],\n",
       "        [ 0.15296464, -1.2160792 ,  0.45247155, ...,  0.9900129 ,\n",
       "         -0.2541578 , -0.9574549 ]],\n",
       "\n",
       "       [[ 0.19869548,  0.17622136,  1.2201    , ...,  1.4798869 ,\n",
       "         -0.32290393,  0.20093115],\n",
       "        [-1.4758697 , -2.3357096 , -1.5564101 , ...,  0.2837574 ,\n",
       "         -1.247156  ,  2.1127117 ],\n",
       "        [ 0.17186421, -0.57441986, -0.25159815, ...,  0.5740387 ,\n",
       "          0.16137673, -1.3327318 ],\n",
       "        ...,\n",
       "        [-1.5181941 , -1.512182  ,  0.4262819 , ..., -0.86411834,\n",
       "         -0.98216414,  0.26931995],\n",
       "        [ 0.8717534 , -2.5395744 ,  1.3015985 , ..., -0.6652781 ,\n",
       "          0.01545628,  0.9050312 ],\n",
       "        [ 0.12468456, -0.43179572,  0.4888643 , ...,  0.39801326,\n",
       "         -0.3984516 , -1.8038934 ]],\n",
       "\n",
       "       [[-0.40699497, -1.1285865 ,  1.5129541 , ...,  1.772464  ,\n",
       "         -1.0481906 ,  0.38361022],\n",
       "        [-1.0252794 , -2.7746336 , -2.2802255 , ...,  0.44931772,\n",
       "         -1.3399125 ,  2.0246055 ],\n",
       "        [ 0.05648304, -1.4532571 ,  0.18493193, ...,  1.4489704 ,\n",
       "         -0.11633615, -1.2943479 ],\n",
       "        ...,\n",
       "        [-1.6138108 , -0.66143405,  0.582769  , ..., -0.08542246,\n",
       "         -0.5468096 ,  1.023799  ],\n",
       "        [-0.17724329, -2.1421566 ,  1.2128755 , ...,  0.15329632,\n",
       "          0.56093043,  1.0174357 ],\n",
       "        [-0.8756345 , -1.1939588 ,  0.00580679, ...,  1.6465311 ,\n",
       "         -0.97497094, -1.2961137 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.3059306 , -0.5299672 ,  1.1108494 , ...,  1.7933204 ,\n",
       "         -1.0397396 ,  0.8334629 ],\n",
       "        [-0.9807199 , -2.6605945 , -1.6711073 , ..., -0.63103515,\n",
       "         -1.4294283 ,  2.0397673 ],\n",
       "        [-0.05403384, -0.41965163,  0.36871207, ...,  0.64264053,\n",
       "         -0.261947  , -1.6350627 ],\n",
       "        ...,\n",
       "        [-1.4566507 , -1.4133072 ,  0.9851854 , ..., -0.81210387,\n",
       "         -0.5987552 ,  0.20666602],\n",
       "        [ 0.91782343, -2.1557345 ,  1.6014144 , ...,  0.5993127 ,\n",
       "          0.1607816 ,  1.7239696 ],\n",
       "        [-0.15716332, -0.96281075, -0.06726392, ...,  1.1775804 ,\n",
       "         -0.8035064 , -0.88329154]],\n",
       "\n",
       "       [[ 0.02943071, -0.7789875 ,  1.3234578 , ...,  1.7261053 ,\n",
       "         -0.25805157,  0.47859773],\n",
       "        [-1.8412197 , -2.400475  , -1.5158159 , ...,  0.07692259,\n",
       "         -0.9421077 ,  1.3822491 ],\n",
       "        [-0.6239116 , -1.6427277 ,  0.55621   , ...,  1.5400655 ,\n",
       "          0.41155022, -1.5380865 ],\n",
       "        ...,\n",
       "        [-2.1268497 , -1.170027  ,  1.2612004 , ...,  0.5372359 ,\n",
       "         -1.0763906 ,  0.55055636],\n",
       "        [ 0.5205984 , -2.2066667 ,  1.4994644 , ..., -0.25006455,\n",
       "          0.28932595,  1.5307645 ],\n",
       "        [ 0.40142673, -1.155224  ,  0.31830072, ...,  0.38355416,\n",
       "         -0.25081375, -1.2911161 ]],\n",
       "\n",
       "       [[ 0.4884136 , -1.1167632 ,  1.4253523 , ...,  2.039767  ,\n",
       "         -0.7461018 ,  1.0178865 ],\n",
       "        [-0.9526967 , -2.5778356 , -2.0231721 , ...,  0.45560753,\n",
       "         -1.3100252 ,  1.973761  ],\n",
       "        [ 0.46675518, -1.5514323 , -0.1069056 , ...,  1.1918527 ,\n",
       "         -0.27022046, -2.0805235 ],\n",
       "        ...,\n",
       "        [-1.1324458 , -0.7622491 ,  0.53443044, ..., -0.77139395,\n",
       "         -0.39579812,  0.49326253],\n",
       "        [ 0.31050667, -2.4049268 ,  1.0431539 , ...,  0.18030906,\n",
       "         -0.40333012,  1.5617077 ],\n",
       "        [ 0.36003107, -0.60908103,  1.0659498 , ...,  1.1037065 ,\n",
       "         -0.6877113 , -0.8984617 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoder = np.random.randint(vocab_size, size = (7, 20))\n",
    "\n",
    "encoder(x_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "906aa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer)    [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_1  (None, 20, 256)     389120      ['decoder_inputs[0][0]']         \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " decoder_state_inputs (InputLay  [(None, 20, 256)]   0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 20, 256)     1578752     ['token_and_position_embedding_1[\n",
      " erDecoder)                                                      0][0]',                          \n",
      "                                                                  'decoder_state_inputs[0][0]']   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 256)      0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20, 1500)     385500      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,353,372\n",
      "Trainable params: 2,353,372\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = keras.Input(\n",
    "    shape = (sequence_length,),\n",
    "    dtype = 'int64', \n",
    "    name = 'decoder_inputs',\n",
    "    )\n",
    "\n",
    "encoder_seq_inputs = keras.Input(\n",
    "    shape = (sequence_length,embed_dim),\n",
    "    name = 'decoder_state_inputs',\n",
    "    )\n",
    "\n",
    "x = nlp_layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocab_size,\n",
    "    sequence_length=sequence_length,\n",
    "    embedding_dim=embed_dim,\n",
    "    )(decoder_inputs)\n",
    "\n",
    "x = nlp_layers.TransformerDecoder(\n",
    "    intermediate_dim = embed_dim,\n",
    "    num_heads = num_heads,\n",
    "    )(x, encoder_seq_inputs)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "decoder_outputs = layers.Dense(\n",
    "    units=vocab_size, activation=\"softmax\"\n",
    "    )(x)\n",
    "\n",
    "decoder = keras.Model(\n",
    "    [decoder_inputs, encoder_seq_inputs],\n",
    "    decoder_outputs\n",
    "    )\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7998c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 20) dtype=int64 (created by layer 'decoder_inputs')>,\n",
       " <KerasTensor: shape=(None, 20, 256) dtype=float32 (created by layer 'decoder_state_inputs')>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05b8c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 20, 1500), dtype=float32, numpy=\n",
       "array([[[0.00071319, 0.00040582, 0.00023623, ..., 0.00065811,\n",
       "         0.00090779, 0.00070867],\n",
       "        [0.00105402, 0.00044944, 0.00097697, ..., 0.00042982,\n",
       "         0.00055488, 0.00012992],\n",
       "        [0.00054012, 0.00037152, 0.00029332, ..., 0.0012637 ,\n",
       "         0.00057611, 0.00031714],\n",
       "        ...,\n",
       "        [0.00042763, 0.00094388, 0.000403  , ..., 0.00032183,\n",
       "         0.00051202, 0.00044714],\n",
       "        [0.00040532, 0.00035947, 0.002458  , ..., 0.00080073,\n",
       "         0.00028309, 0.00095612],\n",
       "        [0.00036497, 0.00072565, 0.00065403, ..., 0.0002926 ,\n",
       "         0.00070892, 0.00055062]],\n",
       "\n",
       "       [[0.00093265, 0.00028962, 0.0002669 , ..., 0.00056267,\n",
       "         0.00064656, 0.0006684 ],\n",
       "        [0.00123452, 0.00037137, 0.00118945, ..., 0.00042334,\n",
       "         0.00068799, 0.0001947 ],\n",
       "        [0.00043764, 0.00033832, 0.00025299, ..., 0.0013302 ,\n",
       "         0.00043955, 0.00032315],\n",
       "        ...,\n",
       "        [0.00030283, 0.00096133, 0.00050102, ..., 0.00045014,\n",
       "         0.00043116, 0.00042912],\n",
       "        [0.0003131 , 0.00028589, 0.00198197, ..., 0.00077746,\n",
       "         0.00025138, 0.00085973],\n",
       "        [0.00045943, 0.00095082, 0.00078705, ..., 0.00027217,\n",
       "         0.00098545, 0.00069012]],\n",
       "\n",
       "       [[0.00098846, 0.00036195, 0.00018534, ..., 0.0005273 ,\n",
       "         0.00076039, 0.00061353],\n",
       "        [0.00085765, 0.00042089, 0.00106873, ..., 0.00078454,\n",
       "         0.0004657 , 0.00014734],\n",
       "        [0.00036162, 0.00031482, 0.00022407, ..., 0.00170544,\n",
       "         0.0006007 , 0.00042771],\n",
       "        ...,\n",
       "        [0.00028018, 0.00089234, 0.00034781, ..., 0.00045869,\n",
       "         0.00038762, 0.00047643],\n",
       "        [0.00025673, 0.00035152, 0.00235384, ..., 0.00076507,\n",
       "         0.00018482, 0.00092397],\n",
       "        [0.00043192, 0.00082201, 0.00093068, ..., 0.00038657,\n",
       "         0.00077417, 0.00049375]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00098136, 0.00040821, 0.0002174 , ..., 0.00068005,\n",
       "         0.00103698, 0.00071229],\n",
       "        [0.00099985, 0.00040051, 0.00085979, ..., 0.00069667,\n",
       "         0.00047443, 0.00014808],\n",
       "        [0.00035152, 0.00030743, 0.00030561, ..., 0.00128142,\n",
       "         0.00044704, 0.00032397],\n",
       "        ...,\n",
       "        [0.00028761, 0.00092319, 0.0003352 , ..., 0.00054093,\n",
       "         0.00057304, 0.00034886],\n",
       "        [0.00032274, 0.00032376, 0.00251146, ..., 0.00071528,\n",
       "         0.00046272, 0.00079032],\n",
       "        [0.00055052, 0.00086146, 0.00074329, ..., 0.00031324,\n",
       "         0.00103259, 0.0004195 ]],\n",
       "\n",
       "       [[0.00162359, 0.00024883, 0.00024727, ..., 0.00068195,\n",
       "         0.00074446, 0.00065133],\n",
       "        [0.00115638, 0.00042227, 0.00108653, ..., 0.00064358,\n",
       "         0.00050256, 0.00019023],\n",
       "        [0.00031594, 0.00034395, 0.00018287, ..., 0.00120068,\n",
       "         0.00055947, 0.00037897],\n",
       "        ...,\n",
       "        [0.00037382, 0.00089342, 0.00031202, ..., 0.00038439,\n",
       "         0.00049827, 0.00050415],\n",
       "        [0.00042115, 0.00028484, 0.00151458, ..., 0.000834  ,\n",
       "         0.00020471, 0.00061409],\n",
       "        [0.00048239, 0.00068469, 0.00072376, ..., 0.00026789,\n",
       "         0.0006707 , 0.00068109]],\n",
       "\n",
       "       [[0.00068001, 0.00042426, 0.00028857, ..., 0.0007849 ,\n",
       "         0.00080231, 0.00070849],\n",
       "        [0.00153108, 0.0004153 , 0.00094976, ..., 0.00063807,\n",
       "         0.00072493, 0.0002281 ],\n",
       "        [0.00037957, 0.00036324, 0.00020515, ..., 0.0016981 ,\n",
       "         0.00045535, 0.00034834],\n",
       "        ...,\n",
       "        [0.00042343, 0.00081567, 0.00050986, ..., 0.00062308,\n",
       "         0.00036821, 0.00045662],\n",
       "        [0.00046612, 0.00019807, 0.00265964, ..., 0.00045457,\n",
       "         0.00022532, 0.00066689],\n",
       "        [0.00043836, 0.00109541, 0.00065415, ..., 0.00031254,\n",
       "         0.00073174, 0.00051783]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_decoder = np.random.randint(vocab_size, size = (7, 20))\n",
    "x_encoder_seq_inputs = np.random.rand(7,20,embed_dim)\n",
    "decoder([x_decoder, x_encoder_seq_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1147b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoder_outputs,        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c69c45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = keras.Model(\n",
    "    [\n",
    "        encoder_inputs,\n",
    "        decoder_inputs,\n",
    "    ],\n",
    "    decoder_outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65e39bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 20, 256)     389120      ['encoder_inputs[0][0]']         \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, 20, 256)     395776      ['token_and_position_embedding[0]\n",
      " erEncoder)                                                      [0]']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 20, 1500)     2353372     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,138,268\n",
      "Trainable params: 3,138,268\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2e8fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "transformer.compile(\n",
    "    'rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80f3a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array([1,3])\n",
    "prediction = np.array([\n",
    "    [0,0.9,0,0.1],\n",
    "    [0.1,0.1,0,0.8],\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    label,\n",
    "    prediction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7626bef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 20) dtype=int64 (created by layer 'encoder_inputs')>,\n",
       " <KerasTensor: shape=(None, 20) dtype=int64 (created by layer 'decoder_inputs')>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f3b4635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 20, 1500) dtype=float32 (created by layer 'model_1')>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9403d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1302/1302 [==============================] - 26s 18ms/step - loss: nan - accuracy: 0.6462 - val_loss: nan - val_accuracy: 0.6456\n",
      "Epoch 2/10\n",
      "1302/1302 [==============================] - 22s 17ms/step - loss: nan - accuracy: 0.6467 - val_loss: nan - val_accuracy: 0.6456\n",
      "Epoch 3/10\n",
      "1302/1302 [==============================] - 22s 17ms/step - loss: nan - accuracy: 0.6467 - val_loss: nan - val_accuracy: 0.6456\n",
      "Epoch 4/10\n",
      "1302/1302 [==============================] - 22s 17ms/step - loss: nan - accuracy: 0.6467 - val_loss: nan - val_accuracy: 0.6456\n",
      "Epoch 5/10\n",
      " 302/1302 [=====>........................] - ETA: 16s - loss: nan - accuracy: 0.6468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.fit(\n",
    "    train_ds,\n",
    "    epochs = epochs,\n",
    "    validation_data=val_ds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb2af563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 20) dtype=int64 (created by layer 'encoder_inputs')>,\n",
       " <KerasTensor: shape=(None, 20) dtype=int64 (created by layer 'decoder_inputs')>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ea99b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 20, 1500), dtype=float32, numpy=\n",
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = transformer([x_encoder, x_decoder])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "054cd610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 20, 1500), dtype=float32, numpy=\n",
       "array([[[0.00072278, 0.0004059 , 0.00023396, ..., 0.00067888,\n",
       "         0.00090478, 0.00070985],\n",
       "        [0.00108344, 0.00044992, 0.00095819, ..., 0.00043884,\n",
       "         0.00055725, 0.00013007],\n",
       "        [0.00054503, 0.00037395, 0.0002882 , ..., 0.00126961,\n",
       "         0.00057907, 0.00031796],\n",
       "        ...,\n",
       "        [0.00043449, 0.00095431, 0.00039809, ..., 0.00032437,\n",
       "         0.00051126, 0.00043628],\n",
       "        [0.00041308, 0.00036313, 0.00238976, ..., 0.00080563,\n",
       "         0.00028065, 0.0009566 ],\n",
       "        [0.00037461, 0.00071781, 0.00064475, ..., 0.00029844,\n",
       "         0.00070316, 0.00053848]],\n",
       "\n",
       "       [[0.00094932, 0.00029017, 0.00025541, ..., 0.00057945,\n",
       "         0.00065116, 0.00067201],\n",
       "        [0.00127722, 0.00037604, 0.00114088, ..., 0.00043365,\n",
       "         0.00070051, 0.00019531],\n",
       "        [0.000437  , 0.00034575, 0.00024672, ..., 0.00133965,\n",
       "         0.00044261, 0.00032507],\n",
       "        ...,\n",
       "        [0.00030118, 0.00097513, 0.00048197, ..., 0.00045625,\n",
       "         0.00042974, 0.00042824],\n",
       "        [0.00032388, 0.00029224, 0.00188066, ..., 0.00078357,\n",
       "         0.00025113, 0.00086645],\n",
       "        [0.00046034, 0.00095153, 0.00076475, ..., 0.00028102,\n",
       "         0.0009927 , 0.00068567]],\n",
       "\n",
       "       [[0.00100066, 0.00036568, 0.00017931, ..., 0.00054088,\n",
       "         0.00074146, 0.00061673],\n",
       "        [0.00087373, 0.00043569, 0.0010337 , ..., 0.000804  ,\n",
       "         0.00047324, 0.00014738],\n",
       "        [0.00036284, 0.00031877, 0.0002173 , ..., 0.00172569,\n",
       "         0.0005973 , 0.00042859],\n",
       "        ...,\n",
       "        [0.00028093, 0.00092145, 0.00033336, ..., 0.00046283,\n",
       "         0.00038804, 0.0004797 ],\n",
       "        [0.0002622 , 0.00035841, 0.00221742, ..., 0.00076299,\n",
       "         0.00018315, 0.00092788],\n",
       "        [0.00043838, 0.00082554, 0.00089644, ..., 0.00040093,\n",
       "         0.00077512, 0.00048842]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00100701, 0.00041581, 0.00021248, ..., 0.00068841,\n",
       "         0.00103111, 0.00071739],\n",
       "        [0.00103057, 0.00040331, 0.00082761, ..., 0.00068923,\n",
       "         0.00047038, 0.00014798],\n",
       "        [0.00035385, 0.00030797, 0.00030227, ..., 0.00126565,\n",
       "         0.00043956, 0.00032245],\n",
       "        ...,\n",
       "        [0.0002899 , 0.00094674, 0.00032173, ..., 0.00053474,\n",
       "         0.00056971, 0.00034516],\n",
       "        [0.00033078, 0.00033088, 0.00243483, ..., 0.00070371,\n",
       "         0.00045611, 0.00079171],\n",
       "        [0.00055381, 0.00087274, 0.00073428, ..., 0.0003176 ,\n",
       "         0.0010334 , 0.00041731]],\n",
       "\n",
       "       [[0.00163749, 0.00025211, 0.00024039, ..., 0.00070511,\n",
       "         0.00074006, 0.0006548 ],\n",
       "        [0.00118141, 0.00043145, 0.00104219, ..., 0.00065974,\n",
       "         0.00051386, 0.00019036],\n",
       "        [0.00031443, 0.00035121, 0.00017776, ..., 0.00123074,\n",
       "         0.00056442, 0.0003783 ],\n",
       "        ...,\n",
       "        [0.00037062, 0.00092198, 0.0003013 , ..., 0.00038757,\n",
       "         0.00049491, 0.00050527],\n",
       "        [0.00042413, 0.00029438, 0.00146087, ..., 0.00084455,\n",
       "         0.00020341, 0.00061385],\n",
       "        [0.00048388, 0.0006831 , 0.00071013, ..., 0.00027734,\n",
       "         0.00067325, 0.00066711]],\n",
       "\n",
       "       [[0.00067986, 0.00043266, 0.00027868, ..., 0.00080159,\n",
       "         0.00081045, 0.00071429],\n",
       "        [0.00155905, 0.00042447, 0.00091853, ..., 0.00064012,\n",
       "         0.00073683, 0.00022746],\n",
       "        [0.00037775, 0.00037319, 0.00020068, ..., 0.00168691,\n",
       "         0.00045669, 0.00034391],\n",
       "        ...,\n",
       "        [0.00042805, 0.00083674, 0.00049474, ..., 0.00062489,\n",
       "         0.00036672, 0.00045158],\n",
       "        [0.00047057, 0.00020057, 0.00256276, ..., 0.00045145,\n",
       "         0.00022257, 0.00066115],\n",
       "        [0.00043968, 0.00111991, 0.00064155, ..., 0.00031504,\n",
       "         0.00073055, 0.00051168]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba4221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53d6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
